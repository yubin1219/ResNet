# -*- coding: utf-8 -*-
"""ResNet50구현.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iOccqr-3sEOq1ZijT_CZqtMNajn8LF70
"""

# Commented out IPython magic to ensure Python compatibility.
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline

x_train, x_test = x_train/255.0 , x_test/255.0

Autotune=tf.data.experimental.AUTOTUNE

def image_preprocess(image,label):
  image_ = tf.keras.layers.experimental.preprocessing.Resizing(224,224)
  return image_,label

train_dataset=tf.data.Dataset.from_tensor_slices((x_train,y_train))

train_dataset=train_dataset.map(image_preprocess,num_parallel_calls=Autotune)

EPOCHS=10
num_classes=10
learning_rate=0.001

class residual_block(tf.keras.Model):
    def __init__(self, filter_out, kernel_size):
        super(residual_block, self).__init__()
        self.conv1 = tf.keras.layers.Conv2D(filter_out,(1,1),(1,1),activation='relu')
        
        self.bn1 = tf.keras.layers.BatchNormalization()
        self.conv2 = tf.keras.layers.Conv2D(filter_out, kernel_size, padding='same',activation='relu')
        
        self.bn2 = tf.keras.layers.BatchNormalization()
        self.conv3 = tf.keras.layers.Conv2D(4*filter_out,(1,1),(1,1),activation='relu')
        
        self.bn3 = tf.keras.layers.BatchNormalization()
        self.shortcut = tf.keras.layers.Conv2D(4*filter_out,(1,1),(1,1),activation='relu')
        self.bn4 = tf.keras.layers.BatchNormalization()

    def call(self, x, training=False, mask=None):
        h=self.conv1(x)
        
        h=self.bn1(h,training=training)
        h=tf.nn.relu(h)
        h=self.conv2(h)
        
        h=self.bn2(h,training=training)
        h=tf.nn.relu(h)
        h=self.conv3(h)
        
        h=self.bn3(h,training=training)

        x_shortcut=self.shortcut(x)
        x_shortcut=self.bn4(x_shortcut,training=training)

        h=tf.keras.layers.Add()([h,x_shortcut])
        h=tf.nn.relu(h)
        
        return h

class identity_block(tf.keras.Model):
    def __init__(self, filter_out, kernel_size):
        super(identity_block, self).__init__()
        self.conv1 = tf.keras.layers.Conv2D(filter_out,(1,1),(1,1),activation='relu')
        
        self.bn1 = tf.keras.layers.BatchNormalization()
        self.conv2 = tf.keras.layers.Conv2D(filter_out, kernel_size, padding='same',activation='relu')
        
        self.bn2 = tf.keras.layers.BatchNormalization()
        self.conv3 = tf.keras.layers.Conv2D(4*filter_out,(1,1),(1,1),activation='relu')
        
        self.bn3 = tf.keras.layers.BatchNormalization()

    def call(self, x, training=False, mask=None):
        h=self.conv1(x)
        
        h=self.bn1(h,training=training)
        h=tf.nn.relu(h)
        h=self.conv2(h)
        
        h=self.bn2(h,training=training)
        h=tf.nn.relu(h)
        h=self.conv3(h)
        
        h=self.bn3(h,training=training)
        
        h=tf.keras.layers.Add()([h,x])
        h=tf.nn.relu(h)
        
        return h

class convolutional_block(tf.keras.Model):
    def __init__(self, filter_out, kernel_size):
        super(convolutional_block, self).__init__()
        self.conv1 = tf.keras.layers.Conv2D(filter_out,(1,1),(2,2),activation='relu')
        
        self.bn1 = tf.keras.layers.BatchNormalization()
        self.conv2 = tf.keras.layers.Conv2D(filter_out, kernel_size, padding='same',activation='relu')
        
        self.bn2 = tf.keras.layers.BatchNormalization()
        self.conv3 = tf.keras.layers.Conv2D(4*filter_out,(1,1),(1,1),activation='relu')
        
        self.bn3 = tf.keras.layers.BatchNormalization()
        
        self.shortcut = tf.keras.layers.Conv2D(4*filter_out,(1,1),(2,2),activation='relu')
        self.bn4 = tf.keras.layers.BatchNormalization()
        
    def call(self, x, training=False, mask=None):
        h=self.conv1(x)
        
        h=self.bn1(h,training=training)
        h=tf.nn.relu(h)
        h=self.conv2(h)
        
        h=self.bn2(h,training=training)
        h=tf.nn.relu(h)
        h=self.conv3(h)
        
        h=self.bn3(h,training=training)
        
        x_shortcut=self.shortcut(x)
        x_shortcut=self.bn4(x_shortcut)
        
        h=tf.keras.layers.Add()([h, x_shortcut])
        h=tf.nn.relu(h)
        
        return h

inputs=tf.keras.layers.Input(shape=(224,224,3))

net=tf.keras.layers.ZeroPadding2D((3,3))(inputs)
net=tf.keras.layers.Conv2D(filters=64,kernel_size=(7,7),strides=(2,2),activation='relu')(net)
net=tf.keras.layers.BatchNormalization()(net)
net=tf.nn.relu(net)
net=tf.keras.layers.ZeroPadding2D((1,1))(net)

net=tf.keras.layers.MaxPooling2D((3,3),(2,2))(net)
net=residual_block(64, (3,3))(net)
net=identity_block(64, (3,3))(net)
net=identity_block(64, (3,3))(net)
        
net=convolutional_block(128, (3,3))(net)
net=identity_block(128, (3,3))(net)
net=identity_block(128, (3,3))(net)
net=identity_block(128, (3,3))(net)
        
net=convolutional_block(256, (3,3))(net)
net=identity_block(256, (3,3))(net)
net=identity_block(256, (3,3))(net)
net=identity_block(256, (3,3))(net)
net=identity_block(256, (3,3))(net)
net=identity_block(256, (3,3))(net)
        
net=convolutional_block(512, (3,3))(net)
net=identity_block(512, (3,3))(net)
net=identity_block(512, (3,3))(net)
        
net=tf.keras.layers.GlobalAveragePooling2D()(net)
        
net=tf.keras.layers.Flatten()(net)
net=tf.keras.layers.Dense(num_classes, activation='softmax')(net)

model=tf.keras.Model(inputs=inputs,outputs=net)

